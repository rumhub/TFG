\chapter{Introducción}

\section{Resumen}
En los inicios, las computadoras empleaban exclusivamente CPUs (Central Processing Units) \cite{CPU_definicion} para llevar a cabo tareas de programación de propósito general. Sin embargo, desde la última década empezaron a surgir otros elementos de procesamiento como las GPUs (Graphics Processing Units) \cite{GPU_definicion}, las cuales se desarrollaron inicialmente para realizar cálculos gráficos paralelos especializados. Con el tiempo, han ido evolucionando tanto en prestaciones como en versatilidad, permitiendo a día de hoy su uso en tareas de cómputo paralelo de propósito general de alto rendimiento. \\
Gracias a la difusión de las GPUs en aplicaciones de propósito general, se logró el cambio de sistemas homogéneos a heterogéneos \cite{sitemas_heterogeneo_definicion}, el cual destaca por ser un logro de gran importancia y considerable magnitud en toda la historia de la computación de alto rendimiento. \\
La computación homogénea emplea uno o más procesadores de la misma arquitectura para ejecutar una aplicación. Por otro lado, la computación heterogénea no se rige por esas reglas y rompe dicha limitación, empleando con ello un conjunto de arquitecturas distintas para ejecutar una misma aplicación, de tal forma que cada arquitectura se encargue de aquellas tareas para las que se encuentra mejor preparada, obteniendo por ello una mejora notable en cuanto a rendimiento. \\
En el campo de la computación heterogénea destaca el uso agrupado de CPUs y GPUs, pues su conjunto forma una excelente complementación. Mientras que la CPU se encuentra optimizada para tareas dinámicas de ráfagas de cómputo cortas y un flujo de control impredecible, la GPU se especializa justamente en el caso contrario: ráfagas de cómputo muy costosas pero con un flujo de control simple. \\
De esta forma, si una tarea presenta un número reducido de datos, una lógica de control sofisticada y un bajo nivel de paralelismo, se asignará a la CPU. Si por el contrario, esta presenta una cantidad exuberante de datos, así como un alto grado de paralelismo al procesar dichos datos, se asignará a la GPU pues presenta un gran número de núcleos y puede dar soporte a una cantidad de hebras mucho mayor que la posible mediante CPU. \cite{Professional_CUDA_C} \\
Tal y como se explicará en detalle en secciones posteriores, el patrón de entrenamiento en redes neuronales convolucionales o CNNs (Convolutional Neural Neworks) \cite{CNN_definicion} es computacionalmente intensivo y altamente paralelo \cite{Programming_Massively}. Por ello, se adoptará un enfoque de computación heterogénea en este ámbito, con el propósito de acortar los tiempos de ejecución requeridos en dichos entrenamientos. Las redes neuronales convolucionales son de gran importancia y destacan por ser un subconjunto del aprendizaje automático \cite{Aprendizaje_automatico_definicion} y el corazón de los algoritmos de aprendizaje profundo \cite{Deep_learning_definicion}, además de potenciar las tareas de reconocimiento de imágenes \cite{image_recognition_CNN} y visión artificial \cite{computer_vision_definicion}. Entre sus principales usos destacan la detección de objetos en imágenes o vídeos \cite{object_detection_CNN}, segmentación de imágenes \cite{image_segmentation_CNN} , generación de imágenes \cite{generative_CNN}, análisis de vídeos \cite{video_analytics_CNN}, procesamiento del lenguaje natural \cite{NLP_CNN}, o incluso sistemas autónomos \cite{sitemas_autonomos_CNN}, entre otros. \\
Con el objetivo de lograr una mayor comprensión sobre sistemas heterogéneos aplicados a redes neuronales convolucionales, a lo largo de este proyecto se desarrollarán una serie de implementaciones. Primero se empezará por una implementación secuencial que simplemente use la CPU. Después, se creará otra implementación que aproveche el paralelismo a nivel de grano grueso en CPU mediante la librería OpenMP \cite{openmp_forum}. Una vez adquiridos conocimientos sobre redes neuronales convolucionales y unas bases paralelismo a nivel de CPU, tendrá lugar la creación de una tercera implementación, caracterizada por ser el primer sistema heterogéneo de este proyecto y emplear el framework CUDA \cite{cuda_forum}. Por último, una vez entendidas las bases de CNNs, paralelismo tanto a nivel de CPU como a nivel de GPU, y sistemas heterogéneos aplicados a CNNs, se contarán con los conocimientos necesarios para crear y entender cómo funcionan realmente por debajo las librerías del sector. Por ello, se seleccionó una librería de bajo nivel y grandes prestaciones como cuDNN \cite{cuDNN} para elaborar una última implementación y lograr obtener un mayor rendimiento, a la vez que se consolida lo aprendido durante todo el proceso. \\
Cabe destacar que cada una de las implementaciones a desarrollar serán de muy bajo nivel y no se apoyarán en ninguna librería o framework externo que facilite los cálculos, con la única excepción de OpenMP, CUDA y cuDNN, tal y como se comentó anteriormente. 


\section{Objetivos}

El principal objetivo de este proyecto es diseñar y desarrollar redes neuronales convolucionales (CNNs) desde sus cimientos, a un nivel de programación relativamente bajo. Esto permite una profunda comprensión de sus fundamentos y funcionamiento, comunes a bibliotecas especializadas en el campo. Para ello, se desarrollan distintas implementaciones de la misma aplicación, cada una con mejores prestaciones que la anterior. \\
La principal razón de este proyecto es aprender los fundamentos del machine learning aplicados a redes neuronales convolucionales, así como el diseño y desarrollo de sistemas heterogéneos de altas prestaciones, y el uso de librerías de bajo nivel del ámbito como cuDNN, que a su vez es empleada por otras librerías de más alto nivel del sector como Caffe2 \cite{Caffe2}, Keras \cite{Keras}, MATLAB \cite{Matlab}, Pytorch \cite{Pytorch}, o TensorFlow \cite{Tensorflow}, entre otras \cite{cuDNN_librerias}. \\
A continuación se desglosan en dos categorías los objetivos específicos que permiten alcanzar el objetivo principal. Los objetivos de aprendizaje se centran en la adquisición de los conocimientos teóricos requeridos para la concepción de este proyecto, mientras que los objetivos de diseño e implementación buscan llevar a la práctica dicho conocimiento teórico adquirido anteriormente, aportando con ello una experiencia de aprendizaje de mayor categoría.

\subsection{Objetivos de aprendizaje}

\begin{enumerate}[label=\textbullet]
	\item \textbf{OA.1} Conocer los fundamentos del machine learning y como se aplican a CNNs.
	
	\item \textbf{OA.2} Conocer los distintos componentes de una CNN y la conexión entre los mismos.
	
	\item \textbf{OA.3} Comprender implementaciones similares a las planteadas en este proyecto para comprender y analizar las funcionalidades y propiedades que se requieren. 
	
	\item \textbf{OA.4} Aprender como diseñar e implementar CNNs empleando tecnologías de programación de bajo nivel como C++.
	
	\item \textbf{OA.5} Comprender el diseño e implementación de CNNs empleando paralelización a nivel de CPU mediante OpenMP y C++.
	
	\item \textbf{OA.6} Mejorar mi aprendizaje sobre diseño e implementación de sistemas heterogéneos usando CUDA.

	\item \textbf{OA.7} Comprender el diseño e implementación de CNNs mediante sistemas heterogéneos usando CUDA.

	\item \textbf{OA.8} Aprender como diseñar e implementar CNNs mediante librerías específicas para este ámbito y de bajo nivel como cuDNN.

\end{enumerate}

\subsection{Objetivos de diseño e implementación}

\begin{enumerate}[label=\textbullet]
	\item \textbf{ODD.1} Diseñar e implementar CNNs a bajo nivel mediante C++.
	
	\item \textbf{ODD.2} Diseñar e implementar CNNs a bajo nivel mediante C++ y paralelización a nivel de CPU mediante OpenMP.

	\item \textbf{ODD.3} Diseñar e implementar CNNs a bajo nivel como sistema heterogéneo mediante C++ y CUDA.

	\item \textbf{ODD.4} Diseñar e implementar CNNs mediante la librería de bajo nivel cuDNN.		
	
\end{enumerate}
