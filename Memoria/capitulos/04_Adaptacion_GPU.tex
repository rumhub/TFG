\chapter{Adaptación GPU}

Una vez comprendidos los cálculos empleados por una CNN, en esta sección se tratará de introducir como se realizarán los mismos desde una perspectiva heterogénea. Esto es, mediante el uso de GPU para multiplicación matricial.

\section{GEMM}

Se trata de un enfoque ampliamente conocido en el mundo de deep learning, empleándolo gran cantidad de librerías del sector como Caffe, Torch-cunn, Theano-CorrMM, o incluso CuDNN \cite{conv_GEMM_FFT_comparacion}. \\
El enfoque GEMM (General Matrix Multiply o Multiplicación General de Matrices) en redes neuronales convolucionales permite reducir el tiempo de cómputo requerido en distintas capas a cambio de aumentar el espacio necesario para la misma. 

\section{Convolución como GEMM \label{Intro_GEMMM}}

En el caso de capas convolucionales, seguir un enfoque GEMM consiste en ``desenrrollar'' tanto el volumen de entrada X como la matriz de filtros K, además de realizar una serie de duplicaciones en X, de tal forma que cada columna de esta nueva matriz 2D X\_unroll contenga todos los elementos de X implicados en el cálculo de una posición distinta del volumen de salida Y. Como los resultados de cada convolución se suman a lo largo de cada canal de profundidad, estos se pueden concatenar en una matriz de gran tamaño. De esta forma, cada kernel de pesos se transforma en una fila de una gran matriz de pesos, tal y como se muestra en la figura \ref{fig:conv_std_vs_gemm} con los kernels K y G (colores verde y marrón). \\
Mientras que el método estándar requiere de varias iteraciones para calcular cada valor del volumen de salida Y (figura \ref{fig:forward_prop_convolucional}), en la parte inferior de la figura \ref{fig:conv_std_vs_gemm} se observa como la alternativa GEMM permite que cada valor de Y sea el resultado de multiplicar una fila de pesos (K o G en la figura \ref{fig:conv_std_vs_gemm}) por una columna de X\_unroll (azul) \cite{Programming_Massively}. \\
De esta forma, con N kernels de tamaño KxK, un volumen de entrada con C canales de profundidad, y un volumen de salida de dimensiones $NxH_{out}xW_{out}$, una multiplicación matricial entre la matriz de pesos $M_1$ con N filas y $K^2*C$ columnas, y X\_unroll con $K^2*C$ filas y $H_{out}*W_{out}$ columnas, produce el mismo volumen de salida Y que una convolución ordinaria con N kernels distintos. \\
Por último, aunque se haya omitido para simplificar la comprensión del método planteado, tras realizar dicha multiplicación matricial se debe sumar el sesgo y aplicar la función de activación correspondiente a cada elemento del volumen de salida Y.

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.35]{imagenes/conv_std_vs_gemm.jpg}  
	\caption{Imagen de una convolución estándar frente a una convolución empleando GEMM}
	\label{fig:conv_std_vs_gemm}
\end{figure}

\subsection{Memoria requerida al emplear GEMM}

Al realizar una convolución, un mismo filtro de pesos puede iterar más de una vez por varios valores de X. Esto implica la duplicación dichos valores de X en la matriz X\_unroll tantas veces como se requiera el acceso a los mismos. En el caso de una entrada X(3x3) y un kernel de pesos K(2x2) (ejemplo de la figura \ref{fig:conv_std_vs_gemm}), el valor central de X deberá duplicarse 4 veces, mientras que el valor central de cada lateral se duplicará 2 veces y los valores de cada esquina solo requieren 1 acceso por lo que no  se duplican. Así, una matriz inicial X(3x3) con 9 valores, se transforma en X\_unroll con 4*1 + 2*4 + 1*4 = 16 valores, contando con un ratio de expansión de 16/9 = 1.8 . \\
 Como cada valor de salida $Y^m_{ij}$ es producto de una convolución de K*K pesos a lo largo de X sobre C canales de profundidad, el número de columnas de X\_unroll se define por C*K*K. De la misma forma, como el resultado de cada fila multiplicada por cada columna de las matrices ``desenrolladas'' aportan un valor $Y^m_{ij}$, X\_unrolled tiene tantas columnas como Y elementos ($M*H_{out}*W_{out}$). \\
 El ratio de expansión se define mediante $\frac{C*K*K*H_{out}*W_{out}}{C*H*W}$, donde H y W hacen referencia a las filas y columnas de X, y $H_{out}$ y $W_{out}$ a las filas y columnas de la salida $Y$ respectivamente. En general, si la entrada X y la salida Y poseen unas dimensiones mucho más grandes que el filtro de pesos K, el ratio de expansión será de K*K. \\
 Dado que cada kernel de pesos presenta unas dimensiones de K*K y se representa como una fila en la matriz total de pesos, esta tendrá K*K filas y M columnas, siendo M el número de filtros de pesos a aplicar sobre X. \\
 Realizar varias convoluciones sobre una misma entrada X(una con cada filtro de pesos distinto) implica compartir una única matriz X\_unrolled. Sin embargo, para valores comunes de filtros de pesos (5 o superior), una matriz de entrada con dimensiones expandidas con un ratio de K*K puede ser excesivamente grande. Como el consumo de memoria por almacenar todas las matrices de entrada expandidas en un minibatch puede suponer un problema, simplemente se reservará un espacio de $C*K*K*H_{out}*W_{out}$ (una instancia de X\_unrolled), de forma que se reutilice para cada dato del minibatch \cite{Programming_Massively}.

\newpage

\section{Retropropagación GEMM en capa convolucional}

En la figura \ref{fig:conv_backprop_como_convolucion_Y_K_pad} se vio como la retropropagación respecto a la entrada en una capa convolucional consistía en una convolución entre los kernels de pesos y el volumen de salida. Del mismo modo, en la figura \ref{fig:conv_backprop_como_convolucion_Xpad_Y} también se mostró como el gradiente respecto a los pesos consistía en una convolución entre el volumen de entrada y el de salida. \\
Como ambos gradientes se pueden implementar como convoluciones, y en la sección \ref{Intro_GEMMM} se mostró como implementar convoluciones con el enfoque GEMM, tiene sentido asumir que la retropropagación en una capa convolucional se puede implementar mediante el enfoque GEMM.

\begin{figure}[H]
	\hspace{-38mm}
	\includegraphics[scale=0.3]{imagenes/conv_std_vs_gemm_backprop.jpg}  
	\caption{Retropropagación en una capa convolucional de forma estándar frente a GEMM respecto a la entrada}
	\label{fig:conv_std_vs_gemm_backprop}
\end{figure}

En la Figura \ref{fig:conv_std_vs_gemm_backprop} cabe destacar que, como cada canal de profundidad c$\in$C de cada kernel solo influye sobre dicho canal c sobre el volumen de entrada X, en la retropropagación sólo recibirá el gradiente proveniente de dicho canal. De esta forma, como cada kernel se empleó para producir un canal m$\in$M distinto sobre el volumen de salida Y, se pueden concatenar los M distintos kernels para un mismo canal de profundidad c, de forma que cada canal c de cada kernel m se multiplique por su subconjunto correspondiente en el canal m de Y e influya sobre el canal c de X (Figura \ref{fig:conv_std_vs_gemm_backprop}). 

\begin{figure}[H]
	\includegraphics[scale=0.33]{imagenes/conv_std_vs_gemm_backprop_pesos.jpg}  
	\caption{Retropropagación en una capa convolucional de forma estándar frente a GEMM respecto a los pesos}
	\label{fig:conv_std_vs_gemm_backprop_pesos}
\end{figure}

En el caso del cálculo del gradiente de la pérdida respecto a los pesos de una capa convolucional, se recuerda que se podía implementar como una convolción entre el volumen de salida y el de entrada. Mediante el enfoque GEMM, se puede observar en la Figura \ref{fig:conv_std_vs_gemm_backprop_pesos} como para ello es necesario `desenrrollar' tanto el volumen de entrada X como el de salida Y. De esta forma, multiplicando cada fila de Y (desenrrollada) por cada columna de X (desenrrollada), se estarán multiplicando todos los elementos del volumen de salida Y y del volumen de entrada X que fueron multiplicados anteriormente (en la propagación hacia delante) por un peso distinto, calculando así el gradiente de dicho peso.




\section{Capa totalmente conectada como GEMM \cite{nvidia_back_fully_GEMM}}

Al igual que las capas convolucionales, las capas totalmente conectadas también se pueden implementar mediante un enfoque GEMM. Para ello, analizaremos por separado cada uno de los 3 casos que nos encontraremos:
\begin{enumerate}[label=\textbullet, nosep]
	\item Propagación hacia delante
	\item Cálculo del gradiente respecto a la entrada
	\item Cálculo del gradiente respecto a los pesos
\end{enumerate}

\subsection{Propagación hacia delante}

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.3]{imagenes/gemm_fully_forward.jpg}  
	\caption{Propagación GEMM hacia delante en una capa totalmente conectada}
	\label{fig:gemm_fully_forward}
\end{figure}

En la Figura \ref{fig:gemm_fully_forward} se muestran 2 formas distintas de implementar la propagación hacia delante en una capa totalmente conectada. El método \textbf{ESTÁNDAR} hace referencia al método empleado en secciones anteriores. Sin embargo, el método \textbf{GEMM} aporta un nuevo ángulo desde el cual afrontarlo. Este último consiste en, para una capa i, agrupar por un lado pesos y sesgos de dicha capa en una matriz, y por otra parte tener otra matriz de una sola columna con todas las neuronas de dicha capa i más un elemento con valor igual a 1 (para sumar el sesgo). \\
Siguiendo la estructura planteada en la Figura \ref{fig:gemm_fully_forward}, cada multiplicación de fila matriz\_pesos\_sesgos por columna de matriz\_neuronas produce como resultado una neurona de la capa i+1 (para completar la propagación hacia delante de dicha capa habría que aplicar la función de activación sobre los resultados obtenidos).

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.25]{imagenes/gemm_fully_forward_minibatch.jpg}  
	\caption{Propagación GEMM de un minibatch entero hacia delante en una capa totalmente conectada}
	\label{fig:gemm_fully_forward_minibatch}
\end{figure}

Si tenemos en cuenta que los pesos y sesgos durante el entrenamiento permanecen constantes durante todo un minibatch, para un minibatch de tamaño N, podemos expandir la matriz matriz\_neuronas definida anteriormente para que tenga N columnas, una por cada elemento del minibatch. De esta forma, mediante una simple multiplicación matricial (enfoque GEMM) se puede realizar la propagación hacia delante de un minibatch entero para una capa totalmente conectada, tal y como se muestra en la Figura \ref{fig:gemm_fully_forward_minibatch} \cite{nvidia_back_fully_GEMM}.

\newpage

\subsection{Retropropagación}

Al igual que la propagación hacia delante, la retropropagación en una capa totalmente conectada también puede se calcular mediante un enfoque de multiplicación matricial como es GEMM.

\subsubsection{Gradiente respecto a la entrada}
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.35]{imagenes/gemm_fully_back_input.jpg}  
	\caption{Cálculo del gradiente respecto a la entrada en una capa totalmente conectada}
	\label{fig:gemm_fully_back_input}
\end{figure}

Suponiendo que que nos encontramos en la capa i y ya tenemos el gradiente de la pérdida respecto a la entrada de la capa i+1, podemos calcular dicho gradiente respecto a la entrada de la capa i tal y como se muestra en la Figura \ref{fig:gemm_fully_back_input}. \\
Esto es, teniendo por un lado una matriz matriz\_pesos donde cada fila j tiene los pesos que conectan a la neurona j de dicha capa i y a cualquier otra neurona de la capa i+1. En este caso, la matriz matriz\_neuronas vuelve a tener una sola columna y contiene las neuronas de la capa i+1. Sin embargo, esta vez dichas neuronas contendrán el gradiente de la pérdida hasta dicha capa. Así, cada multiplicación de una fila de matriz\_pesos por una columna de matriz\_neuronas producirá el cálculo del gradiente de la pérdida respecto a una neurona de entrada distinta. \\
En la Figura \ref{fig:gemm_fully_back_input} se muestra tanto una representación visual de ambas matrices como la fórmula que se emplea en el método estándar, facilitando así una mejor comprensión de que estos dos métodos son simplemente caminos que llevan al mismo destino y por tanto producen los mismos resultados.

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.25]{imagenes/gemm_fully_back_input_minibatch.jpg}  
	\caption{Cálculo del gradiente respecto a la entrada de todo un minibatch en una capa totalmente conectada}
	\label{fig:gemm_fully_back_input_minibatch}
\end{figure}

De forma similar al apartado anterior, para un minibatch de N elementos, se puede expandir la matriz matriz\_neuronas para que contenga N columnas, permitiendo así el cálculo del gradiente de la pérdida respecto a la entrada para una capa i para todo un minibatch mediante una simple multiplicación matricial, tal y como se muestra en la Figura \ref{fig:gemm_fully_back_input_minibatch}.

\subsubsection{Gradiente respecto a los pesos}
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.3]{imagenes/gemm_fully_back_w.jpg}  
	\caption{Cálculo del gradiente respecto a los pesos en una capa totalmente conectada}
	\label{fig:gemm_fully_back_w}
\end{figure}

En este caso, para una capa i, tendremos 2 matrices a las que denominaremos matriz\_entrada y matriz\_salida. La primera de ellas se caracterizará por tener una sola columna con las neuronas de entrada de la capa i, y la segunda contendrá una sola fila con el gradiente de la pérdida respecto a cada neurona de salida de la capa i. De esta forma, cada multiplicación de fila matriz\_entrada por columna de matriz\_salida producirá el cálculo del gradiente de la pérdida respecto a un peso que une una neurona de la capa i con otra de la capa i+1. Es decir, se calcula el gradiente de la pérdida respecto a los pesos de la capa i, tal y como se muestra en la Figura \ref{fig:gemm_fully_back_w}.

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.25]{imagenes/gemm_fully_back_w_minibatch.jpg}  
	\caption{Cálculo del gradiente respecto a los pesos de todo un minibatch en una capa totalmente conectada}
	\label{fig:gemm_fully_back_w_minibatch}
\end{figure}

Para un minibatch de tamaño N, se pueden expandir ambas matrices de forma que estas tengan N filas o columnas (según se indica en la Figura \ref{fig:gemm_fully_back_w_minibatch}) y calcular el gradiente de la pérdida respecto a los pesos para una capa i en un minibatch entero mediante una multiplicación matricial.

\section{CUDA}
CUDA es una plataforma de cómputo paralelo de propósito general y modelo de programación que aprovecha la capacidad computacional de las GPUs de NVIDIA para resolver gran cantidad de problemas computacionalmente costosos de manera eficiente. CUDA permite acceder a la GPU de forma similar al común acceso a CPU que experimenta un desarrollador promedio. \\
El modelo de programación CUDA permite ejecutar aplicaciones en sistemas de computación heterogéneos, caracterizándose estos por CPUs y GPUs, cada uno con su propia memoria separada por un bus PCI-Express. \\
Cuando se lanza una función kernel desde CPU, la ejecución se traslada a GPU, esta genera un gran número de hebras y cada una de ellas ejecuta las órdenes especificadas en dicho kernel. \\
Las hebras se organizan en una cuadrícula o grid compuesta por varios bloques de hebras. De esta forma, cada hebra pertenece a un bloque de hebras, y cada bloque de un mismo kernel pertenece a un mismo grid. Todas las hebras de un mismo grid comparten la misma memoria global. Por tanto, hebras de distintos grids no pueden cooperar. A su vez, las hebras de un mismo bloque se pueden sincronizar y compartir memoria a nivel de bloque, siendo esta más escasa pero presentando una latencia considerablemente menor que la memoria global. CUDA organiza los grids y bloques mediante estructuras que pueden ser 1D, 2D o incluso 3D \cite{Professional_CUDA_C}.
\subsection{Multiplicación de matrices en CUDA}

Dadas dos matrices A(MxK) y B(KxN), se obtiene C(KxN) como producto de la multiplicación AB. Basándome en mi propia experiencia, una primera aproximación de cara a realizar dicha operación en CUDA consiste en crear un bloque Block(KxN), de forma que cada hebra calcule una posición de la matriz resultado C. Dada la naturaleza extremadamente simple de tal implementación, el desarrollador que la llevó a cabo optará por buscar formas de reducir el tiempo de cómputo requerido por la misma, optando en la mayoría de los casos por emplear memoria compartida de bloque. Tras desarrollar esta ``segunda versión'', el siguiente paso del desarrollador consiste en comparar ambas implementaciones para verificar si efectivamente se obtiene una ganancia notable sobre la primera. En el proceso se observa como para matrices de tamaño reducido ambas implementaciones parecen funcionar. Sin embargo, a medida que aumenta el tamaño de las mismas se ve un claro ``tope'' pues ambas comparten un mismo defecto, el tamaño de bloque. Por ejemplo, para calcular C(50x50) = A(50x10) x B(10x50) ambas implementaciones requerirían un bloque de 50x50 = 2500 hebras, lo cual es imposible pues excede el tamaño límite (CUDA solo permite 1024 hebras por bloque). \\

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.3]{imagenes/gemm_tile_v3.jpg}  
	\caption{Tercera implementación de multiplicación matricial con CUDA}
	\label{fig:mult_matrix_cuda_v3}
\end{figure}

Por tanto, resulta evidente la existencia de una ``tercera'' implementación carente de este gran defecto. Esta se caracteriza por la división la matriz C en submatrices o tiles, de forma que cada tile sea un bloque cuda. Partiendo de las implementaciones anteriores, el cambio resulta inmediato y los resultados descomunales pues aporta la capacidad de multiplicación matricial con independencia del tamaño de A y B, además de mejoras en cuanto a rendimiento \cite{cuda_mult_matrix_v3}. \\
Sin embargo, esta última implementación carece de los beneficios de la memoria compartida nivel de bloque, por lo que una vez más indica la muy posible existencia de una mejora sobre la misma. \\

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.3]{imagenes/gemm_tile_v4.jpg}  
	\caption{Cuarta implementación de multiplicación matricial con CUDA}
	\label{fig:mult_matrix_cuda_v4}
\end{figure}
Para realizar el cálculo de cada valor de C es necesario multiplicar una fila de A por una columna de B. Es decir, multiplicar dos vectores de K elementos. Una idea inicial consiste en cargar 2K elementos en memoria compartida para posteriormente usarlos. Sin embargo, si cada hebra de cada bloque requiere 2K elementos, esto no es viable pues la memoria compartida es limitada y escasa. En su lugar, un mejor enfoque consiste en, dado un bloque 2D de dimensiones TILExTILE, almacenar 2*TILE*TILE. Su objetivo es dividir el cálculo de cada valor de C en iteraciones, y en cada una de ellas, cada hebra multiplica TILE elementos de A por TILE de B, además de acumular y sumar los resultados obtenidos en cada iteración \cite{nvidia_mult_matrix_v4}. \\
