\chapter{Conclusiones y trabajo futuro}

Tras la comparación de las distintas implementaciones para redes neuronales convolucionales realizada a lo largo de este proyecto, se concluye que la implementación que utiliza cuDNN es la más eficiente. Aunque todas las implementaciones producen resultados equivalentes y logran el aprendizaje esperado, cuDNN destaca por su capacidad para reducir los timepos de cómputo en comparación con las demás opciones. Esta eficiencia es una de las razones por las que cuDNN se emplea en numerosas librerías de alto nivel, como TensorFlow o Caffe2, entre otras. Además, se han cumplido los objetivos planteados al inicio del proyecto de manera satisfactoria. \\
Con respecto al trabajo futuro que se podría realizar, se destacan la siguientes áreas de mejora
\begin{enumerate}[label=\textbullet]
	\item \textbf{Optimización de la implementación en CUDA}: Mejora de la implementación en CUDA para reducir la brecha en los tiempos ded cómputo en comparación con cuDNN.
	\item \textbf{Evaluación de librerías de alto nivel}: Realizar implementaciones adicionales de redes neuronales convolucionales utilizando librerías de alto nivel y comparar los resultados con las implementaciones existentes en este proyecto.
	\item \textbf{Investigación de cuDNN a bajo nivel}: Explorar el funcionamiento interno de cuDNN y tratar de optimizarlo en casos específicos para mejorar su rendimiento.
	\item \textbf{Comparación con librerías de alto nivel que emplean cuDNN}: Analizar el rendimiento de cuDNN en comparación con las librerías de alto nivel que lol utilizan como componente subyacente.
	\item \textbf{Desarrollo de sistemas heterogéneos optimizados}: Utilizar librerías de bajo nivel, como cuDNN, para desarrollar sistemas heterogéneos optimizados en algoritmos de aprendizaje profundo más complejos que las redes neuronales convolcionales.  
\end{enumerate}