@book{Programming_Massively,
	title     = "Programming Massively Parallel Processors",
	author    = "Wen-emi W.Hwu, David B.kirk, Izzat El Hajj",
	publisher = "Morgan Kaufmann",
	address = "50 Hampshire Street, 5th Floor, Cambridge, MA 02139, United States",
	edition = "4",
	year      = "2022",
}

@book{Learning_From_Data,
	title     = "Learning From Data",
	author    = "Yaser S. Abu-Mostafa, Malik Magdon-Ismail, Hsuan-Tien Lin",
	publisher = " ",
	address = "California Institute of Technology Pasadena, CA 91125, USA",
	edition = "1",
	year      = "2012",
}

@misc{SoftMax_MLM,
	title        = {Softmax Activation Function with Python},
	author       = {Jason Brownlee},
	year         = 2020,
	note         = {\url{https://machinelearningmastery.com/softmax-activation-function-with-python/} [Accessed:24/02/2024]}
}

@book{NN_From_Scratch,
	title     = "Neural Networks From Scratch in Python",
	author    = "Harrison Kinsley & Daniel Kukieła",
	publisher = " ",
	address = "Sentdex, Kinsley Enterprises Inc",
	edition = "1",
	year      = "2020",
}

@misc{ReLU,
	title        = {Unidad Lineal Rectificada},
	author       = {Douglas Karr },
	year         = 2024,
	note         = {\url{https://es.martech.zone/acronym/relu/} [Accessed:25/02/2024]}
}

@misc{Sigmoide,
	title        = {La Función Sigmoide: Una Herramienta Clave en Redes Neuronales},
	author       = {Javi},
	year         = 2023,
	note         = {\url{https://jacar.es/la-funcion-sigmoide-una-herramienta-clave-en-redes-neuronales/}  [Accessed:25/02/2024]}
}

@misc{SGD_1,
	title        = {Descenso del gradiente},
	author       = {},
	year         = 2024,
	note         = {\url{https://es.wikipedia.org/wiki/Descenso_del_gradiente#:~:text=El%20descenso%20del%20gradiente%20o,en%20direcci%C3%B3n%20contraria%20al%20gradiente.}  [Accessed:26/02/2024]}
}

@misc{SGD_2,
	title        = {Gradient Descent Algorithm — a deep dive},
	author       = {Robert Kwiatkowski},
	year         = 2021,
	note         = {\url{https://towardsdatascience.com/gradient-descent-algorithm-a-deep-dive-cf04e8115f21#:~:text=Gradient%20descent%20(GD)%20is%20an,e.g.%20in%20a%20linear%20regression).}  [Accessed:26/02/2024]}
}

@misc{Gradiente,
	title        = {Gradiente},
	author       = {},
	year         = 2024,
	note         = {\url{https://es.wikipedia.org/wiki/Gradiente}  [Accessed:26/02/2024]}
}

@misc{SGD_act_params,
	title        = {How neural networks are trained},
	author       = {ml4a},
	year         =  2020,
	note         = {\url{https://ml4a.github.io/ml4a/how_neural_networks_are_trained/}  [Accessed:27/02/2024]}
}

@misc{Cross_entropy,
	title        = {What Is Cross Entropy Loss? A Tutorial With Code},
	author       = {Saurav Maheshkar},
	year         =  2023,
	note         = {\url{https://wandb.ai/sauravmaheshkar/cross-entropy/reports/What-Is-Cross-Entropy-Loss-A-Tutorial-With-Code--VmlldzoxMDA5NTMx#:~:text=Cross%20entropy%20loss%20is%20a,close%20to%200%20as%20possible.}  [Accessed:29/02/2024]}
}

@misc{Cross_entropy_backprop,
	title        = {Back-propagation through Cross-Entropy Softmax},
	author       = {mehran@mldawn.com},
	year         =  2021,
	note         = {\url{https://www.mldawn.com/back-propagation-with-cross-entropy-and-softmax/}  [Accessed:29/02/2024]}
}

@misc{Cross_entropy_backprop_grad_input,
	title        = {The derivative of softmax function wrt z},
	author       = {mehran@mldawn.com},
	year         =  2021,
	note         = {\url{https://www.mldawn.com/the-derivative-of-softmaxz-function-w-r-t-z/}  [Accessed:05/03/2024]}
}

@misc{NN_backpropagation,
	title        = {Back-Propagation is very simple. Who made it Complicated ?},
	author       = {Prakash Jay},
	year         =  2017,
	note         = {\url{https://medium.com/@14prakash/back-propagation-is-very-simple-who-made-it-complicated-97b794c97e5c}  [Accessed:06/03/2024]}
}

@misc{NN_backprop_2,
	title        = {No more confusion on Backpropagation},
	author       = {Chamanth mvs},
	year         =  2022,
	note         = {\url{https://pub.aimind.so/no-more-confusion-on-backpropagation-7adfc271539f}  [Accessed:07/03/2024]}
}
