@book{Programming_Massively,
	title     = "Programming Massively Parallel Processors",
	author    = "Wen-emi W.Hwu, David B.kirk, Izzat El Hajj",
	publisher = "Morgan Kaufmann",
	address = "50 Hampshire Street, 5th Floor, Cambridge, MA 02139, United States",
	edition = "4",
	year      = "2022",
}

@book{Professional_CUDA_C,
	title     = "Professional Cuda C Programming",
	author    = "John Cheng, Max Grossman, Ty McKercher",
	publisher = "John Wiley and Sons, Inc.",
	address = "10475 Crosspoint Boulevard",
	edition = "1",
	year      = "2014",
}

@book{Learning_From_Data,
	title     = "Learning From Data",
	author    = "Yaser S. Abu-Mostafa, Malik Magdon-Ismail, Hsuan-Tien Lin",
	publisher = " ",
	address = "California Institute of Technology Pasadena, CA 91125, USA",
	edition = "1",
	year      = "2012",
}

@misc{SoftMax_MLM,
	title        = {Softmax Activation Function with Python},
	author       = {Jason Brownlee},
	year         = 2020,
	note         = {\url{https://machinelearningmastery.com/softmax-activation-function-with-python/} [Accessed:24/02/2024]}
}

@book{NN_From_Scratch,
	title     = "Neural Networks From Scratch in Python",
	author    = "Harrison Kinsley & Daniel Kukieła",
	publisher = " ",
	address = "Sentdex, Kinsley Enterprises Inc",
	edition = "1",
	year      = "2020",
}

@misc{ReLU,
	title        = {Unidad Lineal Rectificada},
	author       = {Douglas Karr },
	year         = 2024,
	note         = {\url{https://es.martech.zone/acronym/relu/} [Accessed:25/02/2024]}
}

@misc{Sigmoide,
	title        = {La Función Sigmoide: Una Herramienta Clave en Redes Neuronales},
	author       = {Javi},
	year         = 2023,
	note         = {\url{https://jacar.es/la-funcion-sigmoide-una-herramienta-clave-en-redes-neuronales/}  [Accessed:25/02/2024]}
}

@misc{SGD_1,
	title        = {Descenso del gradiente},
	author       = {},
	year         = 2024,
	note         = {\url{https://es.wikipedia.org/wiki/Descenso_del_gradiente#:~:text=El%20descenso%20del%20gradiente%20o,en%20direcci%C3%B3n%20contraria%20al%20gradiente.}  [Accessed:26/02/2024]}
}

@misc{SGD_2,
	title        = {Gradient Descent Algorithm — a deep dive},
	author       = {Robert Kwiatkowski},
	year         = 2021,
	note         = {\url{https://towardsdatascience.com/gradient-descent-algorithm-a-deep-dive-cf04e8115f21#:~:text=Gradient%20descent%20(GD)%20is%20an,e.g.%20in%20a%20linear%20regression).}  [Accessed:26/02/2024]}
}

@misc{SGD_3,
	title        = {How is stochastic gradient descent implemented in the context of machine learning and deep learning?},
	author       = {Sebastian Raschka},
	year         = 2024,
	note         = {\url{https://sebastianraschka.com/faq/docs/sgd-methods.html}  [Accessed:27/03/2024]}
}

@misc{Gradiente,
	title        = {Gradiente},
	author       = {},
	year         = 2024,
	note         = {\url{https://es.wikipedia.org/wiki/Gradiente}  [Accessed:26/02/2024]}
}

@misc{SGD_act_params,
	title        = {How neural networks are trained},
	author       = {ml4a},
	year         =  2020,
	note         = {\url{https://ml4a.github.io/ml4a/how_neural_networks_are_trained/}  [Accessed:27/02/2024]}
}

@misc{Cross_entropy,
	title        = {What Is Cross Entropy Loss? A Tutorial With Code},
	author       = {Saurav Maheshkar},
	year         =  2023,
	note         = {\url{https://wandb.ai/sauravmaheshkar/cross-entropy/reports/What-Is-Cross-Entropy-Loss-A-Tutorial-With-Code--VmlldzoxMDA5NTMx#:~:text=Cross%20entropy%20loss%20is%20a,close%20to%200%20as%20possible.}  [Accessed:29/02/2024]}
}

@misc{Cross_entropy_backprop,
	title        = {Back-propagation through Cross-Entropy Softmax},
	author       = {mehran@mldawn.com},
	year         =  2021,
	note         = {\url{https://www.mldawn.com/back-propagation-with-cross-entropy-and-softmax/}  [Accessed:29/02/2024]}
}

@misc{Cross_entropy_backprop_grad_input,
	title        = {The derivative of softmax function wrt z},
	author       = {mehran@mldawn.com},
	year         =  2021,
	note         = {\url{https://www.mldawn.com/the-derivative-of-softmaxz-function-w-r-t-z/}  [Accessed:05/03/2024]}
}

@misc{NN_backpropagation,
	title        = {Back-Propagation is very simple. Who made it Complicated ?},
	author       = {Prakash Jay},
	year         =  2017,
	note         = {\url{https://medium.com/@14prakash/back-propagation-is-very-simple-who-made-it-complicated-97b794c97e5c}  [Accessed:06/03/2024]}
}

@misc{NN_backprop_2,
	title        = {No more confusion on Backpropagation},
	author       = {Chamanth mvs},
	year         =  2022,
	note         = {\url{https://pub.aimind.so/no-more-confusion-on-backpropagation-7adfc271539f}  [Accessed:07/03/2024]}
}

@misc{ini_He,
	title        = {Weight Initialization for Deep Learning Neural Networks},
	author       = {Jason Brownlee},
	year         =  2021,
	note         = {\url{https://machinelearningmastery.com/weight-initialization-for-deep-learning-neural-networks/}  [Accessed:14/03/2024]}
}

@misc{ini_He_2,
	title        = {Kaiming Initialization in Deep Learning},
	author       = {Sandeep Jain},
	year         =  2023,
	note         = {\url{https://www.geeksforgeeks.org/kaiming-initialization-in-deep-learning/ }  [Accessed:14/03/2024]}
}

@misc{ini_He_code,
	title        = {Understanding weight initialization for neural networks},
	author       = {Adrian Rosebrock},
	year         =  2021,
	note         = {\url{https://pyimagesearch.com/2021/05/06/understanding-weight-initialization-for-neural-networks/}  [Accessed:14/03/2024]}
}

@misc{ini_bias,
	title        = {Initial bias values for a neural network},
	author       = {Yahia Zakaria},
	year         =  2017,
	note         = {\url{https://stackoverflow.com/questions/44883861/initial-bias-values-for-a-neural-network}  [Accessed:14/03/2024]}
}

@misc{ini_bias_2,
	title        = {Bias Initialization in a Neural Network},
	author       = {Glen Meyerowitz},
	year         =  2018,
	note         = {\url{https://medium.com/@glenmeyerowitz/bias-initialization-in-a-neural-network-2e5d26fed0f0}  [Accessed:14/03/2024]}
}

@misc{capa_convolucional,
	title        = {Convolutional Neural Networks cheatsheet},
	author       = {Afshine Amidi y Shervine Amidi},
	year         =  2018,
	note         = {\url{https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-convolutional-neural-networks}  [Accessed:19/03/2024]}
}

@misc{capa_convolucional_Stanford,
	title        = {Convolutional Neural Networks (CNNs / ConvNets)},
	author       = {Stanford},
	year         =  2020,
	note         = {\url{https://cs231n.github.io/convolutional-networks/}  [Accessed:19/03/2024]}
}

@misc{sgd_stocastico,
	title        = {Stochastic gradient descent},
	author       = {Wikipedia},
	year         =  2024,
	note         = {\url{https://en.wikipedia.org/wiki/Stochastic_gradient_descent}  [Accessed:22/03/2024]}
}

@misc{sgd_stocastico_1,
	title        = {Stochastic Gradient Descent — Clearly Explained !!},
	author       = {
	Aishwarya V Srinivasan},
	year         =  2019,
	note         = {\url{https://towardsdatascience.com/stochastic-gradient-descent-clearly-explained-53d239905d31}  [Accessed:22/03/2024]}
}

@inproceedings{CNN_parallel_Stanford,
	title={Parallel and Distributed Deep Learning},
	author={Vishakh Hegde and Sheema Usmani},
	year={2016},
	url={https://api.semanticscholar.org/CorpusID:53135767}
}

@INPROCEEDINGS{CNN_parallel_International_Conference,
	author={Lee, Sunwoo and Jha, Dipendra and Agrawal, Ankit and Choudhary, Alok and Liao, Wei-keng},
	booktitle={2017 IEEE 24th International Conference on High Performance Computing (HiPC)}, 
	title={Parallel Deep Convolutional Neural Network Training by Exploiting the Overlapping of Computation and Communication}, 
	year={2017},
	volume={},
	number={},
	pages={183-192},
	keywords={Training;Computational modeling;Data models;Backpropagation;Neurons;Mathematical model;Convolution;Convolutional Neural Network;Deep Learning;Parallelization;Communication;Overlapping},
	doi={10.1109/HiPC.2017.00030}}

@article{CNN_parallel_Ome_Weird_Trick,
	title={One weird trick for parallelizing convolutional neural networks},
	author={Alex Krizhevsky},
	journal={ArXiv},
	year={2014},
	volume={abs/1404.5997},
	url={https://api.semanticscholar.org/CorpusID:5556470}
}

@misc{flatten_forward,
	title        = {Let’s Code Convolutional Neural Network in plain NumPy},
	author       = {Piotr Skalski},
	year         =  2020,
	note         = {\url{https://towardsdatascience.com/lets-code-convolutional-neural-network-in-plain-numpy-ce48e732f5d5}  [Accessed:24/03/2024]}
}

@misc{max_pool_backprop,
	title        = {Backprop Through Max-Pooling Layers?},
	author       = {Archana David},
	year         =  2007,
	note         = {\url{https://datascience.stackexchange.com/questions/11699/backprop-through-max-pooling-layers}  [Accessed:24/03/2024]}
}

@misc{max_pool_backprop_2,
	title        = {How to backpropagate through max-pooling layers},
	author       = {Muhammad Baqir},
	year         =  2024,
	note         = {\url{https://www.educative.io/answers/how-to-backpropagate-through-max-pooling-layers}  [Accessed:24/03/2024]}
}

@misc{conv_backprop,
	title        = {Calculate CNN backprop with padding and stride set to 2},
	author       = {Hide Inada},
	year         =  2024,
	note         = {\url{https://hideyukiinada.github.io/cnn_backprop_strides2.html}  [Accessed:01/04/2024]}
}

@misc{padding_1,
	title        = {CNN | Introduction to Padding},
	author       = {
	savyakhosla},
	year         =  2024,
	note         = {\url{https://www.geeksforgeeks.org/cnn-introduction-to-padding/}  [Accessed:02/04/2024]}
}

@inbook{padding_2,
	author = {Kumar, Avinash and Sarkar, Sobhangi and Pradhan, Chittaranjan},
	year = {2020},
	month = {01},
	pages = {211-230},
	title = {Malaria Disease Detection Using CNN Technique with SGD, RMSprop and ADAM Optimizers},
	isbn = {978-3-030-33965-4},
	doi = {10.1007/978-3-030-33966-1_11}
}

@article{conv_GEMM_FFT_comparacion,
	title={Performance Analysis of GPU-based Convolutional
	Neural Networks},
	author={Xiaqing Li†‡§, Guangyan Zhang†‡§, H. Howie Huang¶, Zhufan Wang†‡, Weimin Zheng†‡
	†Department of Computer Science and Technology, Tsinghua University
	‡Tsinghua National Laboratory for Information Science and Technology
	§State Key Lab of Mathematical Engineering and Advanced Computing, Wuxi, China
	¶Department of Electrical and Computer Engineering, George Washington University},
	journal={International Conference on Parallel Processing},
	year={2016},
	volume={ },
	url={https://www2.seas.gwu.edu/~howie/publications/GPU-CNN-ICPP16.pdf}
}

@misc{cuda_mult_matrix_v3,
	title        = {COMP 605: Introduction to Parallel Computing
	Lecture : CUDA Matrix-Matrix Multiplication},
	author       = {Mary Thomas},
	year         =  2017,
	note         = {\url{https://edoras.sdsu.edu/~mthomas/sp17.605/lectures/CUDA-Mat-Mat-Mult.pdf}  [Accessed:10/05/2024]}
}

@misc{nvidia_mult_matrix_v4,
	title        = {Matrix Multiplication Background,  User's Guide | NVIDIA Docs},
	author       = {NVIDIA},
	year         =  2023,
	note         = {\url{https://docs.nvidia.com/deeplearning/performance/pdf/Matrix-Multiplication-Background-User-Guide.pdf}  [Accessed:10/05/2024]}
}

@misc{nvidia_back_fully_GEMM,
	title        = {Linear/Fully-Connected Layers User's Guide},
	author       = {NVIDIA},
	year         =  2024,
	note         = {\url{https://docs.nvidia.com/deeplearning/performance/dl-performance-fully-connected/index.html}  [Accessed:11/05/2024]}
}