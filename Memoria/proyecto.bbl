\begin{thebibliography}{10}

\bibitem{Cross_entropy_backprop}
mehran@mldawn.com.
\newblock Back-propagation through cross-entropy softmax, 2021.
\newblock
  \url{https://www.mldawn.com/back-propagation-with-cross-entropy-and-softmax/}
  [Accessed:29/02/2024].

\bibitem{Cross_entropy_backprop_grad_input}
mehran@mldawn.com.
\newblock The derivative of softmax function wrt z, 2021.
\newblock
  \url{https://www.mldawn.com/the-derivative-of-softmaxz-function-w-r-t-z/}
  [Accessed:05/03/2024].

\bibitem{NN_backpropagation}
Prakash Jay.
\newblock Back-propagation is very simple. who made it complicated ?, 2017.
\newblock
  \url{https://medium.com/@14prakash/back-propagation-is-very-simple-who-made-it-complicated-97b794c97e5c}
  [Accessed:06/03/2024].

\bibitem{NN_backprop_2}
Chamanth mvs.
\newblock No more confusion on backpropagation, 2022.
\newblock
  \url{https://pub.aimind.so/no-more-confusion-on-backpropagation-7adfc271539f}
  [Accessed:07/03/2024].

\bibitem{nvidia_back_fully_GEMM}
NVIDIA.
\newblock Linear/fully-connected layers user's guide, 2024.
\newblock
  \url{https://docs.nvidia.com/deeplearning/performance/dl-performance-fully-connected/index.html}
  [Accessed:11/05/2024].

\bibitem{CPU_definicion}
Amazon.
\newblock ¿qué es una unidad central de procesamiento (cpu)?, 2024.
\newblock \url{https://aws.amazon.com/es/what-is/cpu/} [Accessed:15/08/2024].

\bibitem{GPU_definicion}
Amazon.
\newblock ¿qué es una gpu?, 2024.
\newblock \url{https://aws.amazon.com/es/what-is/gpu/} [Accessed:15/08/2024].

\bibitem{sitemas_heterogeneo_definicion}
Patricio Bulić.
\newblock Heterogeneous systems, 2024.
\newblock
  \url{https://doc.sling.si/en/workshops/programming-gpu-cuda/01-intro/02-hetsys/#:~:text=A%20homogeneous%20system%20uses%20one,%2Dsuited%2C%20yielding%20performance%20improvement.}
  [Accessed:15/08/2024].

\bibitem{Professional_CUDA_C}
Ty~McKercher John~Cheng, Max~Grossman.
\newblock {\em Professional Cuda C Programming}.
\newblock John Wiley and Sons, Inc., 10475 Crosspoint Boulevard, 1 edition,
  2014.

\bibitem{CNN_definicion}
IBM.
\newblock What are convolutional neural networks?, 2024.
\newblock \url{https://www.ibm.com/topics/convolutional-neural-networks}
  [Accessed:15/08/2024].

\bibitem{Programming_Massively}
Izzat El~Hajj Wen-emi W.Hwu, David~B.kirk.
\newblock {\em Programming Massively Parallel Processors}.
\newblock Morgan Kaufmann, 50 Hampshire Street, 5th Floor, Cambridge, MA 02139,
  United States, 4 edition, 2022.

\bibitem{Aprendizaje_automatico_definicion}
IBM.
\newblock ¿qué es el machine learning (ml)?, 2024.
\newblock \url{https://www.ibm.com/es-es/topics/machine-learning}
  [Accessed:15/08/2024].

\bibitem{Deep_learning_definicion}
IBM.
\newblock ¿qué es el deep learning?, 2024.
\newblock \url{https://www.ibm.com/es-es/topics/deep-learning}
  [Accessed:15/08/2024].

\bibitem{image_recognition_CNN}
Mohdsanadzakirizvi.
\newblock Image classification using cnn, 2024.
\newblock
  \url{https://www.analyticsvidhya.com/blog/2020/02/learn-image-classification-cnn-convolutional-neural-networks-3-datasets/}
  [Accessed:15/08/2024].

\bibitem{computer_vision_definicion}
Microsoft.
\newblock ¿qué es la visión artificial?, 2024.
\newblock
  \url{https://azure.microsoft.com/es-es/resources/cloud-computing-dictionary/what-is-computer-vision#clasificaci%C3%B3n-de-objetos}
  [Accessed:15/08/2024].

\bibitem{object_detection_CNN}
Yi~Wang Jusong~Ren.
\newblock Overview of object detection algorithms using convolutional neural
  networks, 2024.
\newblock \url{https://www.scirp.org/journal/paperinformation?paperid=115011}
  [Accessed:15/08/2024].

\bibitem{image_segmentation_CNN}
Shameerayaseen.
\newblock U-net: Advancing image segmentation with convolutional neural
  networks, 2023.
\newblock
  \url{https://medium.com/@shameerayaseen21/u-net-advancing-image-segmentation-with-convolutional-neural-networks-1fd810f05d00}
  [Accessed:15/08/2024].

\bibitem{generative_CNN}
Ying Nian~Wu Jifeng~Dai, Yang~Lu.
\newblock Generative modeling of convolutional neural networks, 2023.
\newblock \url{http://www.stat.ucla.edu/~ywu/generativeCNN/main.html}
  [Accessed:15/08/2024].

\bibitem{video_analytics_CNN}
Intel.
\newblock Convolutional neural networks (cnns), deep learning, and computer
  vision, 2024.
\newblock
  \url{https://www.intel.com/content/www/us/en/internet-of-things/computer-vision/convolutional-neural-networks.html}
  [Accessed:15/08/2024].

\bibitem{NLP_CNN}
Taha Binhuraib.
\newblock Nlp with cnns, 2020.
\newblock \url{https://towardsdatascience.com/nlp-with-cnns-a6aa743bdc1e}
  [Accessed:15/08/2024].

\bibitem{sitemas_autonomos_CNN}
Augmented AI.
\newblock Convolutional neural networks (cnn) in self-driving cars, 2024.
\newblock
  \url{https://www.augmentedstartups.com/blog/convolutional-neural-networks-cnn-in-self-driving-cars#:~:text=They%20have%20the%20potential%20to,decisions%20based%20on%20the%20environment.}
  [Accessed:15/08/2024].

\bibitem{openmp_forum}
OpeMP.
\newblock Openmp, 2024.
\newblock \url{https://www.openmp.org/} [Accessed:15/08/2024].

\bibitem{cuda_forum}
NVIDIA.
\newblock Cuda toolkit, 2024.
\newblock \url{https://developer.nvidia.com/cuda-toolkit}
  [Accessed:15/08/2024].

\bibitem{cuDNN}
NVIDIA.
\newblock Nvidia cudnn, 2024.
\newblock \url{https://developer.nvidia.com/cudnn} [Accessed:02/08/2024].

\bibitem{Caffe2}
Caffe2.
\newblock What is caffe2?, 2024.
\newblock \url{https://caffe2.ai/docs/caffe-migration.html}
  [Accessed:15/08/2024].

\bibitem{Keras}
Keras.
\newblock Keras, 2024.
\newblock \url{https://keras.io/} [Accessed:15/08/2024].

\bibitem{Matlab}
Mathworks.
\newblock Matlab, 2024.
\newblock \url{https://la.mathworks.com/products/matlab.html}
  [Accessed:15/08/2024].

\bibitem{Pytorch}
Pytorch.
\newblock Pytorch, 2024.
\newblock \url{https://pytorch.org/} [Accessed:15/08/2024].

\bibitem{Tensorflow}
Tensorflow.
\newblock Tensorflow, 2024.
\newblock \url{https://www.tensorflow.org/?hl=es-419} [Accessed:15/08/2024].

\bibitem{cuDNN_librerias}
NVIDIA.
\newblock Nvidia cudnn, 2024.
\newblock
  \url{https://docs.nvidia.com/deeplearning/cudnn/latest/developer/overview.html#dev-overview}
  [Accessed:02/08/2024].

\bibitem{Learning_From_Data}
Hsuan-Tien~Lin Yaser S. Abu-Mostafa, Malik Magdon-Ismail.
\newblock {\em Learning From Data}.
\newblock California Institute of Technology Pasadena, CA 91125, USA, 1
  edition, 2012.

\bibitem{aprendizaje_supervisado}
IBM.
\newblock ¿qué es el aprendizaje supervisado?, 2024.
\newblock \url{https://www.ibm.com/es-es/topics/supervised-learning}
  [Accessed:22/08/2024].

\bibitem{etiquetas}
Interactive Chaos.
\newblock ¿qué es el aprendizaje supervisado?, 2024.
\newblock
  \url{https://interactivechaos.com/es/manual/tutorial-de-machine-learning/etiquetas}
  [Accessed:22/08/2024].

\bibitem{NN_intro}
IBM.
\newblock ¿qué son las redes neuronales?, 2024.
\newblock
  \url{https://www.ibm.com/es-es/topics/neural-networks#:~:text=%C2%BFQu%C3%A9%20son%20las%20redes%20neuronales,opciones%20y%20llegar%20a%20conclusiones.}
  [Accessed:10/08/2024].

\bibitem{funcion_activacion_definicion}
Sakshi Tiwari.
\newblock Funciones de activación en redes neuronales, 2024.
\newblock
  \url{https://www.geeksforgeeks.org/activation-functions-neural-networks/}
  [Accessed:10/08/2024].

\bibitem{ReLU}
Douglas Karr.
\newblock Unidad lineal rectificada, 2024.
\newblock \url{https://es.martech.zone/acronym/relu/} [Accessed:25/02/2024].

\bibitem{importancia_ReLU}
Jorge~Calvo Martin.
\newblock La importancia de las funciones de activación en una red neuronal,
  2022.
\newblock
  \url{https://www.linkedin.com/pulse/la-importancia-de-las-funciones-activaci%C3%B3n-en-una-red-calvo-martin/}
  [Accessed:10/06/2024].

\bibitem{importancia_ReLU_2}
Miguel Sotaquirá.
\newblock La función de activación, 2018.
\newblock \url{https://www.codificandobits.com/blog/funcion-de-activacion/}
  [Accessed:10/06/2024].

\bibitem{Sigmoide}
Javi.
\newblock La función sigmoide: Una herramienta clave en redes neuronales,
  2023.
\newblock
  \url{https://jacar.es/la-funcion-sigmoide-una-herramienta-clave-en-redes-neuronales/}
  [Accessed:25/02/2024].

\bibitem{SoftMax_MLM}
Jason Brownlee.
\newblock Softmax activation function with python, 2020.
\newblock
  \url{https://machinelearningmastery.com/softmax-activation-function-with-python/}
  [Accessed:24/02/2024].

\bibitem{one_hot}
Interactive Chaos.
\newblock One hot encoding, 2024.
\newblock
  \url{https://interactivechaos.com/es/manual/tutorial-de-machine-learning/one-hot-encoding}
  [Accessed:22/08/2024].

\bibitem{Cross_entropy}
Saurav Maheshkar.
\newblock What is cross entropy loss? a tutorial with code, 2023.
\newblock
  \url{https://wandb.ai/sauravmaheshkar/cross-entropy/reports/What-Is-Cross-Entropy-Loss-A-Tutorial-With-Code--VmlldzoxMDA5NTMx#:~:text=Cross%20entropy%20loss%20is%20a,close%20to%200%20as%20possible.}
  [Accessed:29/02/2024].

\bibitem{SGD_1}
Descenso del gradiente, 2024.
\newblock
  \url{https://es.wikipedia.org/wiki/Descenso_del_gradiente#:~:text=El%20descenso%20del%20gradiente%20o,en%20direcci%C3%B3n%20contraria%20al%20gradiente.}
  [Accessed:26/02/2024].

\bibitem{Gradiente}
Gradiente, 2024.
\newblock \url{https://es.wikipedia.org/wiki/Gradiente} [Accessed:26/02/2024].

\bibitem{SGD_2}
Robert Kwiatkowski.
\newblock Gradient descent algorithm — a deep dive, 2021.
\newblock
  \url{https://towardsdatascience.com/gradient-descent-algorithm-a-deep-dive-cf04e8115f21#:~:text=Gradient%20descent%20(GD)%20is%20an,e.g.%20in%20a%20linear%20regression).}
  [Accessed:26/02/2024].

\bibitem{ini_He}
Jason Brownlee.
\newblock Weight initialization for deep learning neural networks, 2021.
\newblock
  \url{https://machinelearningmastery.com/weight-initialization-for-deep-learning-neural-networks/}
  [Accessed:14/03/2024].

\bibitem{ini_He_2}
Sandeep Jain.
\newblock Kaiming initialization in deep learning, 2023.
\newblock
  \url{https://www.geeksforgeeks.org/kaiming-initialization-in-deep-learning/ }
  [Accessed:14/03/2024].

\bibitem{ini_He_code}
Adrian Rosebrock.
\newblock Understanding weight initialization for neural networks, 2021.
\newblock
  \url{https://pyimagesearch.com/2021/05/06/understanding-weight-initialization-for-neural-networks/}
  [Accessed:14/03/2024].

\bibitem{ini_bias}
Yahia Zakaria.
\newblock Initial bias values for a neural network, 2017.
\newblock
  \url{https://stackoverflow.com/questions/44883861/initial-bias-values-for-a-neural-network}
  [Accessed:14/03/2024].

\bibitem{ini_bias_2}
Glen Meyerowitz.
\newblock Bias initialization in a neural network, 2018.
\newblock
  \url{https://medium.com/@glenmeyerowitz/bias-initialization-in-a-neural-network-2e5d26fed0f0}
  [Accessed:14/03/2024].

\bibitem{SGD_act_params}
ml4a.
\newblock How neural networks are trained, 2020.
\newblock \url{https://ml4a.github.io/ml4a/how_neural_networks_are_trained/}
  [Accessed:27/02/2024].

\bibitem{SGD_3}
Sebastian Raschka.
\newblock How is stochastic gradient descent implemented in the context of
  machine learning and deep learning?, 2024.
\newblock \url{https://sebastianraschka.com/faq/docs/sgd-methods.html}
  [Accessed:27/03/2024].

\bibitem{sgd_stocastico}
Wikipedia.
\newblock Stochastic gradient descent, 2024.
\newblock \url{https://en.wikipedia.org/wiki/Stochastic_gradient_descent}
  [Accessed:22/03/2024].

\bibitem{sgd_stocastico_1}
Aishwarya~V Srinivasan.
\newblock Stochastic gradient descent — clearly explained !!, 2019.
\newblock
  \url{https://towardsdatascience.com/stochastic-gradient-descent-clearly-explained-53d239905d31}
  [Accessed:22/03/2024].

\bibitem{capa_convolucional}
Afshine~Amidi y~Shervine~Amidi.
\newblock Convolutional neural networks cheatsheet, 2018.
\newblock
  \url{https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-convolutional-neural-networks}
  [Accessed:19/03/2024].

\bibitem{capa_convolucional_Stanford}
Stanford.
\newblock Convolutional neural networks (cnns / convnets), 2020.
\newblock \url{https://cs231n.github.io/convolutional-networks/}
  [Accessed:19/03/2024].

\bibitem{padding_definicion}
DeepAI.
\newblock Understanding padding in machine learning, 2024.
\newblock \url{https://deepai.org/machine-learning-glossary-and-terms/padding}
  [Accessed:23/08/2024].

\bibitem{padding_1}
savyakhosla.
\newblock Cnn | introduction to padding, 2024.
\newblock \url{https://www.geeksforgeeks.org/cnn-introduction-to-padding/}
  [Accessed:02/04/2024].

\bibitem{full_padding_definicion}
Avinash Kumar.
\newblock Full padding, 2020.
\newblock
  \url{https://www.researchgate.net/figure/Full-padding-and-same-padding-10_fig5_337287600}
  [Accessed:23/08/2024].

\bibitem{padding_2}
Avinash Kumar, Sobhangi Sarkar, and Chittaranjan Pradhan.
\newblock {\em Malaria Disease Detection Using CNN Technique with SGD, RMSprop
  and ADAM Optimizers}, pages 211--230.
\newblock 01 2020.

\bibitem{max_pool_backprop}
Archana David.
\newblock Backprop through max-pooling layers?, 2007.
\newblock
  \url{https://datascience.stackexchange.com/questions/11699/backprop-through-max-pooling-layers}
  [Accessed:24/03/2024].

\bibitem{max_pool_backprop_2}
Muhammad Baqir.
\newblock How to backpropagate through max-pooling layers, 2024.
\newblock
  \url{https://www.educative.io/answers/how-to-backpropagate-through-max-pooling-layers}
  [Accessed:24/03/2024].

\bibitem{flatten_forward}
Piotr Skalski.
\newblock Let’s code convolutional neural network in plain numpy, 2020.
\newblock
  \url{https://towardsdatascience.com/lets-code-convolutional-neural-network-in-plain-numpy-ce48e732f5d5}
  [Accessed:24/03/2024].

\bibitem{openmp_intro}
Arnab Chakraborty.
\newblock What is openmp?, 2019.
\newblock \url{https://www.tutorialspoint.com/what-is-openmp}
  [Accessed:24/08/2024].

\bibitem{openmp_directivas}
Microsoft.
\newblock Directivas de openmp, 2024.
\newblock
  \url{https://learn.microsoft.com/es-es/cpp/parallel/openmp/reference/openmp-directives?view=msvc-170}
  [Accessed:24/08/2024].

\bibitem{cuda_kernels}
NVIDIA.
\newblock Cuda c++ programming guide, 2024.
\newblock
  \url{https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html}
  [Accessed:24/08/2024].

\bibitem{cuDNN_core_concepts}
NVIDIA.
\newblock Nvidia cudnn, 2024.
\newblock
  \url{https://docs.nvidia.com/deeplearning/cudnn/latest/developer/core-concepts.html}
  [Accessed:02/08/2024].

\bibitem{cuDNN_conv_fwd}
NVIDIA.
\newblock Nvidia cudnn, 2024.
\newblock
  \url{https://docs.nvidia.com/deeplearning/cudnn/latest/api/cudnn-cnn-library.html#cudnnconvolutionforward}
  [Accessed:03/08/2024].

\bibitem{cuDNN_pool_fwd}
NVIDIA.
\newblock Nvidia cudnn, 2024.
\newblock
  \url{https://docs.nvidia.com/deeplearning/cudnn/archives/cudnn-897/api/index.html#cudnnPoolingForward}
  [Accessed:03/08/2024].

\bibitem{cuDNN_conv_back_w}
NVIDIA.
\newblock Nvidia cudnn, 2024.
\newblock
  \url{https://docs.nvidia.com/deeplearning/cudnn/latest/api/cudnn-cnn-library.html#cudnnconvolutionbackwardfilter}
  [Accessed:03/08/2024].

\bibitem{cuDNN_conv_back_x}
NVIDIA.
\newblock Nvidia cudnn, 2024.
\newblock
  \url{https://docs.nvidia.com/deeplearning/cudnn/latest/api/cudnn-cnn-library.html#cudnnconvolutionbackwarddata}
  [Accessed:03/08/2024].

\bibitem{CNN_parallel_Stanford}
Vishakh Hegde and Sheema Usmani.
\newblock Parallel and distributed deep learning.
\newblock 2016.

\bibitem{CNN_parallel_International_Conference}
Sunwoo Lee, Dipendra Jha, Ankit Agrawal, Alok Choudhary, and Wei-keng Liao.
\newblock Parallel deep convolutional neural network training by exploiting the
  overlapping of computation and communication.
\newblock In {\em 2017 IEEE 24th International Conference on High Performance
  Computing (HiPC)}, pages 183--192, 2017.

\bibitem{CNN_parallel_Ome_Weird_Trick}
Alex Krizhevsky.
\newblock One weird trick for parallelizing convolutional neural networks.
\newblock {\em ArXiv}, abs/1404.5997, 2014.

\bibitem{conv_backprop}
Hide Inada.
\newblock Calculate cnn backprop with padding and stride set to 2, 2024.
\newblock \url{https://hideyukiinada.github.io/cnn_backprop_strides2.html}
  [Accessed:01/04/2024].

\bibitem{conv_GEMM_FFT_comparacion}
H.~Howie Huang¶ Zhufan Wang†‡ Weimin Zheng†‡ †Department of
  Computer~Science Xiaqing~Li†‡§, Guangyan~Zhang†‡§, Tsinghua
  University ‡Tsinghua National Laboratory for Information~Science
  Technology, Technology §State Key~Lab of~Mathematical~Engineering, China
  ¶Department of~Electrical Advanced~Computing, Wuxi, and George
  Washington~University Computer~Engineering.
\newblock Performance analysis of gpu-based convolutional neural networks.
\newblock {\em International Conference on Parallel Processing}, 2016.

\bibitem{cuda_mult_matrix_v3}
Mary Thomas.
\newblock Comp 605: Introduction to parallel computing lecture : Cuda
  matrix-matrix multiplication, 2017.
\newblock
  \url{https://edoras.sdsu.edu/~mthomas/sp17.605/lectures/CUDA-Mat-Mat-Mult.pdf}
  [Accessed:10/05/2024].

\bibitem{nvidia_mult_matrix_v4}
NVIDIA.
\newblock Matrix multiplication background, user's guide | nvidia docs, 2023.
\newblock
  \url{https://docs.nvidia.com/deeplearning/performance/pdf/Matrix-Multiplication-Background-User-Guide.pdf}
  [Accessed:10/05/2024].

\end{thebibliography}
