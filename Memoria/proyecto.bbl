\begin{thebibliography}{10}

\bibitem{Programming_Massively}
Izzat El~Hajj Wen-emi W.Hwu, David~B.kirk.
\newblock {\em Programming Massively Parallel Processors}.
\newblock Morgan Kaufmann, 50 Hampshire Street, 5th Floor, Cambridge, MA 02139,
  United States, 4 edition, 2022.

\bibitem{Learning_From_Data}
Hsuan-Tien~Lin Yaser S. Abu-Mostafa, Malik Magdon-Ismail.
\newblock {\em Learning From Data}.
\newblock California Institute of Technology Pasadena, CA 91125, USA, 1
  edition, 2012.

\bibitem{ReLU}
Douglas Karr.
\newblock Unidad lineal rectificada, 2024.
\newblock \url{https://es.martech.zone/acronym/relu/} [Accessed:25/02/2024].

\bibitem{Sigmoide}
Javi.
\newblock La función sigmoide: Una herramienta clave en redes neuronales,
  2023.
\newblock
  \url{https://jacar.es/la-funcion-sigmoide-una-herramienta-clave-en-redes-neuronales/}
  [Accessed:25/02/2024].

\bibitem{SoftMax_MLM}
Jason Brownlee.
\newblock Softmax activation function with python, 2020.
\newblock
  \url{https://machinelearningmastery.com/softmax-activation-function-with-python/}
  [Accessed:24/02/2024].

\bibitem{Cross_entropy}
Saurav Maheshkar.
\newblock What is cross entropy loss? a tutorial with code, 2023.
\newblock
  \url{https://wandb.ai/sauravmaheshkar/cross-entropy/reports/What-Is-Cross-Entropy-Loss-A-Tutorial-With-Code--VmlldzoxMDA5NTMx#:~:text=Cross%20entropy%20loss%20is%20a,close%20to%200%20as%20possible.}
  [Accessed:29/02/2024].

\bibitem{SGD_1}
Descenso del gradiente, 2024.
\newblock
  \url{https://es.wikipedia.org/wiki/Descenso_del_gradiente#:~:text=El%20descenso%20del%20gradiente%20o,en%20direcci%C3%B3n%20contraria%20al%20gradiente.}
  [Accessed:26/02/2024].

\bibitem{Gradiente}
Gradiente, 2024.
\newblock \url{https://es.wikipedia.org/wiki/Gradiente} [Accessed:26/02/2024].

\bibitem{SGD_2}
Robert Kwiatkowski.
\newblock Gradient descent algorithm — a deep dive, 2021.
\newblock
  \url{https://towardsdatascience.com/gradient-descent-algorithm-a-deep-dive-cf04e8115f21#:~:text=Gradient%20descent%20(GD)%20is%20an,e.g.%20in%20a%20linear%20regression).}
  [Accessed:26/02/2024].

\bibitem{SGD_act_params}
ml4a.
\newblock How neural networks are trained, 2020.
\newblock \url{https://ml4a.github.io/ml4a/how_neural_networks_are_trained/}
  [Accessed:27/02/2024].

\bibitem{Cross_entropy_backprop}
mehran@mldawn.com.
\newblock Back-propagation through cross-entropy softmax, 2021.
\newblock
  \url{https://www.mldawn.com/back-propagation-with-cross-entropy-and-softmax/}
  [Accessed:29/02/2024].

\bibitem{Cross_entropy_backprop_grad_input}
mehran@mldawn.com.
\newblock The derivative of softmax function wrt z, 2021.
\newblock
  \url{https://www.mldawn.com/the-derivative-of-softmaxz-function-w-r-t-z/}
  [Accessed:05/03/2024].

\end{thebibliography}
